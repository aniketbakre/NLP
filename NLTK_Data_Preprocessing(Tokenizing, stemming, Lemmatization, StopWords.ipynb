{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc1578d8",
   "metadata": {},
   "source": [
    "# NLTK PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd484678",
   "metadata": {},
   "source": [
    "# Tokanization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c22ff20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\anikbakr\\appdata\\local\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\anikbakr\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in c:\\users\\anikbakr\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anikbakr\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\anikbakr\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\anikbakr\\appdata\\local\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5914ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt') Use it if any error using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ac134d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My name is aniket.', 'I am from sindhudurg.', 'I am going to my hotetown']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "data=\"My name is aniket. I am from sindhudurg. I am going to my hotetown\"\n",
    "tokens=nltk.sent_tokenize(data) # Sentence Token\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae1255a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'aniket', '.', 'I', 'am', 'from', 'sindhudurg', '.', 'I', 'am', 'going', 'to', 'my', 'hotetown']\n"
     ]
    }
   ],
   "source": [
    "tokens2=nltk.word_tokenize(data) # Word Token\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edad79ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokens)),\n",
    "print(type(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ededeec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# Because it is class of list we can check the count of the tokens by using length fun.\n",
    "print(len(tokens))\n",
    "print(len(tokens2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7d16ad",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc90cf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Import library\n",
    "from nltk.stem import PorterStemmer\n",
    "#store the porterstemmer in ps\n",
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b1c0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My : my\n",
      "name : name\n",
      "is : is\n",
      "aniket : aniket\n",
      ". : .\n",
      "I : i\n",
      "am : am\n",
      "from : from\n",
      "sindhudurg : sindhudurg\n",
      ". : .\n",
      "I : i\n",
      "am : am\n",
      "going : go\n",
      "to : to\n",
      "my : my\n",
      "hotetown : hotetown\n"
     ]
    }
   ],
   "source": [
    "# Use for loop\n",
    "for i in tokens2:\n",
    "    print(i,\":\",ps.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8558e1a",
   "metadata": {},
   "source": [
    "**-Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma. <br>-For instance, stemming the word 'Caring' would return 'Car'. <br>-For instance, lemmatizing the word 'Caring' would return 'Care'. <br>-Stemming is used in case of large dataset where performance is an issue.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711ccb5f",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "107d3455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anikbakr\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f20d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules and store it\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmati=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61c12216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\anikbakr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4') #use that if omw-1.4 not found in librarry and showing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ed1deeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My : My\n",
      "name : name\n",
      "is : is\n",
      "aniket : aniket\n",
      ". : .\n",
      "I : I\n",
      "am : am\n",
      "from : from\n",
      "sindhudurg : sindhudurg\n",
      ". : .\n",
      "I : I\n",
      "am : am\n",
      "going : going\n",
      "to : to\n",
      "my : my\n",
      "hotetown : hotetown\n"
     ]
    }
   ],
   "source": [
    "#use for loop\n",
    "lem_tokens2=[]\n",
    "for i in tokens2:\n",
    "    print(i,\":\",lemmati.lemmatize(i)),\n",
    "    lem_tokens2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b214db6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'aniket', '.', 'I', 'am', 'from', 'sindhudurg', '.', 'I', 'am', 'going', 'to', 'my', 'hotetown']\n"
     ]
    }
   ],
   "source": [
    "print(lem_tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c863c882",
   "metadata": {},
   "source": [
    "# Remove Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6783313",
   "metadata": {},
   "source": [
    "**With nltk you don’t have to define every stop word manually. \n",
    "<br>Stop words are frequently used words that carry very little meaning. \n",
    "<br>Stop words are words that are so common they are basically ignored by typical tokenizers.\n",
    "<br>By default, NLTK (Natural Language Toolkit) includes a list of 40 stop words, including: “a”, “an”, “the”, “of”, “in”, etc.\n",
    "<br>The stopwords in nltk are the most common words in data. \n",
    "<br>They are words that you do not want to use to describe the topic of your content.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd226ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6b89d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anikbakr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "181e0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store stopwords in the variable from english language\n",
    "stop_words=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41a03f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'should', 'with', 't', 'having', 'before', \"you're\", \"couldn't\", 'didn', 'hasn', 'does', 'ma', \"shan't\", 'theirs', 'won', 'doing', 'to', 've', 'by', 'mustn', 'your', 'very', 'which', 'whom', 'be', 'or', 'this', 'have', 'again', 'out', \"won't\", 'she', \"that'll\", 'are', 'himself', 'under', 'some', 'they', 'because', 'hers', 'them', 'nor', 'both', 'isn', 'myself', 'yourselves', \"you've\", 'needn', \"shouldn't\", 'for', 'don', 'other', \"mustn't\", \"wasn't\", 'those', 'wouldn', \"hadn't\", 'once', 'mightn', 'has', 'am', 'any', 'was', 'over', 'hadn', \"wouldn't\", 're', 'of', 'a', 'our', 'o', 'me', 'his', 'same', 'so', \"doesn't\", \"she's\", 'too', \"aren't\", 'wasn', 'her', 'ours', 'who', 'did', 'on', 'shan', 'and', 'while', 'at', 'ourselves', 'yourself', 'each', 'yours', 'shouldn', 'doesn', 'most', 'few', \"don't\", 'into', 'been', 'herself', 'there', 'above', \"needn't\", 'more', \"should've\", \"isn't\", 'haven', \"haven't\", \"you'd\", 'down', 'now', 'ain', 'my', 'off', 'why', 'between', 'it', 'is', 'being', 'further', \"you'll\", 'will', 'just', 'an', 'had', 'weren', 'after', 'through', 'aren', 'he', 'm', 'when', 'about', 'up', 'below', 'y', 'couldn', 'but', 'him', 'we', 'against', 'no', 'here', 'you', 'd', 'these', 'can', 'than', \"weren't\", 'all', 'where', 'i', 'own', 'its', 'until', 'only', 'll', 'during', 'themselves', \"mightn't\", \"hasn't\", 'that', 'their', 'if', \"didn't\", 's', 'were', 'in', 'such', \"it's\", 'do', 'as', 'from', 'not', 'then', 'what', 'the', 'how', 'itself'}\n"
     ]
    }
   ],
   "source": [
    "# Check stopwords\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8f5276ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n",
      "am\n",
      "from\n",
      "am\n",
      "to\n",
      "my\n"
     ]
    }
   ],
   "source": [
    "# Using for loop to find stop words\n",
    "for i in lem_tokens2:\n",
    "    if i in stop_words:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "80d6472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from tokens2/data\n",
    "tokens3=[]\n",
    "for i in lem_tokens2:\n",
    "    if not i in stop_words:\n",
    "        tokens3.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d4d61623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'name', 'aniket', '.', 'I', 'sindhudurg', '.', 'I', 'going', 'hotetown']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6844b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
